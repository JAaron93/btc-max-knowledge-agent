#!/usr/bin/env python3\n"""\nDirect unit tests for SecurityValidator component.\n"""\n\nimport asyncio\nimport sys\n\n# Use test utilities for robust import handling\nfrom test_utils import (\n    TestAssertion,\n    get_test_config,\n    print_test_header,\n    setup_test_imports,\n    validate_security_imports,\n)\n\n# Set up imports robustly\nif not setup_test_imports():\n# sys.exit(1) disabled to avoid aborting pytest collection\nif not validate_security_imports():\n    pass  # added to satisfy indentation\n# sys.exit(1) disabled to avoid aborting pytest collection\nfrom security.models import SecurityAction, SecuritySeverity, SecurityViolation\n\n# Now we can safely import the security modules\nfrom security.validator import LibraryHealthStatus, SecurityValidator\n\n\nasync def run_tests():\n    """Run all SecurityValidator tests."""\n\n    print_test_header(\n        "SecurityValidator Unit Tests",\n        "Direct unit tests for SecurityValidator component",\n    )\n\n    # Setup using test utilities\n    config = get_test_config()\n    if config is None:\n        print("❌ Failed to create test configuration")\n        return False\n\n    validator = SecurityValidator(config)\n    test = TestAssertion()\n\n    def assert_dangerous_patterns_removed(\n        original_input: str,\n        sanitized_output: str,\n        dangerous_patterns: list,\n        test_description: str,\n    ):\n        """\n        Helper method to strictly validate that dangerous patterns are neutralized or removed.\n\n        This method checks that dangerous patterns are either:\n        1. Completely removed from the output\n        2. Properly escaped/encoded to be harmless\n        3. Significantly reduced in count (indicating partial sanitization)\n\n        Args:\n            original_input: The original input string\n            sanitized_output: The sanitized output string\n            dangerous_patterns: List of dangerous patterns that must be neutralized\n            test_description: Description for test reporting\n        """\n        original_lower = original_input.lower()\n        sanitized_lower = sanitized_output.lower()\n\n        for pattern in dangerous_patterns:\n            pattern_lower = pattern.lower()\n            original_count = original_lower.count(pattern_lower)\n            sanitized_count = sanitized_lower.count(pattern_lower)\n\n            if original_count > 0:\n                # Pattern was present in original, check if it's been neutralized\n                pattern_neutralized = (\n                    sanitized_count == 0  # Completely removed\n                    or sanitized_count < original_count  # Reduced count\n                    or _is_pattern_escaped(\n                        pattern, original_input, sanitized_output\n                    )  # Properly escaped\n                )\n\n                test.assert_test(\n                    pattern_neutralized,\n                    f"{test_description}: '{pattern}' neutralized (removed/escaped/reduced)",\n                )\n\n        # Ensure dangerous executable content is neutralized\n        _assert_no_executable_content(\n            original_input, sanitized_output, test_description\n        )\n\n    def _is_pattern_escaped(pattern: str, original: str, sanitized: str) -> bool:\n        """Check if a dangerous pattern has been properly escaped."""\n        # Common escaping patterns\n        escaped_chars = {\n            "<": ["&lt;", "&lt", "&#60;", "&#x3c;"],\n            ">": ["&gt;", "&gt", "&#62;", "&#x3e;"],\n            '"': ["&quot;", "&#34;", "&#x22;"],\n            "'": ["&#39;", "&#x27;"],\n            "&": ["&amp;", "&#38;", "&#x26;"],\n        }\n\n        # Check if dangerous characters in the pattern are escaped\n        for char, escapes in escaped_chars.items():\n            if char in pattern and any(escape in sanitized for escape in escapes):\n                return True\n\n        return False\n\n    def _assert_no_executable_content(\n        original: str, sanitized: str, test_description: str\n    ):\n        """Assert that no executable content remains in sanitized output."""\n        # Check for common executable patterns that should never remain unescaped\n        executable_patterns = [\n            "<script",\n            "javascript:",\n            "onload=",\n            "onclick=",\n            "onerror=",\n            "eval(",\n        ]\n\n        sanitized_lower = sanitized.lower()\n        for pattern in executable_patterns:\n            if pattern in original.lower():\n                # If pattern was in original, it should not remain unescaped\n                test.assert_test(\n                    pattern not in sanitized_lower or "&" in sanitized,\n                    f"{test_description}: Executable pattern '{pattern}' neutralized",\n                )\n\n    # Test 1: Initialization\n    test.assert_test(validator.config is not None, "SecurityValidator initialization")\n    test.assert_test(\n        isinstance(validator.library_health, LibraryHealthStatus),\n        "Library health status initialization",\n    )\n    test.assert_test(\n        len(validator._compiled_patterns) == len(SecurityValidator.HIGH_RISK_PATTERNS),\n        "Pattern compilation",\n    )\n\n    # Test 2: Library status information via public API\n    library_status = validator.get_library_status()\n    test.assert_test(\n        isinstance(library_status, dict), "Library status returns dictionary"\n    )\n    test.assert_test(\n        "libinjection" in library_status, "Library status includes libinjection"\n    )\n    test.assert_test("bleach" in library_status, "Library status includes bleach")\n    test.assert_test(\n        "markupsafe" in library_status, "Library status includes markupsafe"\n    )\n\n    # Verify structure of library status entries\n    for lib_name in ["libinjection", "bleach", "markupsafe"]:\n        if lib_name in library_status:\n            lib_info = library_status[lib_name]\n            test.assert_test(\n                isinstance(lib_info, dict), f"{lib_name} info is dictionary"\n            )\n            test.assert_test(\n                "available" in lib_info, f"{lib_name} has availability info"\n            )\n\n    # Test 3: Input length validation - within limit\n    test_input = "a" * 1024  # 1KB\n    context = {"source_ip": "127.0.0.1"}\n    result = await validator.validate_input(test_input, context)\n    length_violations = [\n        v for v in result.violations if v.violation_type == "input_length_exceeded"\n    ]\n    test.assert_test(\n        len(length_violations) == 0, "Input within length limit passes validation"\n    )\n\n    # Test 4: Input length validation - exceeds limit\n    test_input = "a" * 5000  # 5KB, exceeds 4KB limit\n    result = await validator.validate_input(test_input, context)\n    length_violations = [\n        v for v in result.violations if v.violation_type == "input_length_exceeded"\n    ]\n    test.assert_test(\n        len(length_violations) == 1, "Input exceeding length limit fails validation"\n    )\n    test.assert_test(\n        length_violations[0].severity == SecuritySeverity.ERROR,\n        "Length violation has ERROR severity",\n    )\n    test.assert_test(\n        length_violations[0].confidence_score == 1.0,\n        "Length violation has high confidence",\n    )\n\n    # Test 5: Boundary conditions\n    test_input_at_limit = "a" * 4096  # Exactly at limit\n    result_at_limit = await validator.validate_input(test_input_at_limit, context)\n    length_violations_at_limit = [\n        v\n        for v in result_at_limit.violations\n        if v.violation_type == "input_length_exceeded"\n    ]\n    test.assert_test(\n        len(length_violations_at_limit) == 0, "Input exactly at limit passes validation"\n    )\n\n    test_input_over_limit = "a" * 4097  # One byte over\n    result_over_limit = await validator.validate_input(test_input_over_limit, context)\n    length_violations_over_limit = [\n        v\n        for v in result_over_limit.violations\n        if v.violation_type == "input_length_exceeded"\n    ]\n    test.assert_test(\n        len(length_violations_over_limit) == 1,\n        "Input one byte over limit fails validation",\n    )\n\n    # Test 6: Fallback pattern detection\n    test_cases = [\n        ("<script>alert('xss')</script>", "script_tag_injection"),\n        ("'; DROP TABLE users; --", "sql_drop_injection"),\n        ("$(document).ready(function(){});", "jquery_injection"),\n        ("{{user.name}}", "template_injection"),\n        ("`rm -rf /`", "backtick_injection"),\n        ("test\x00null", "null_byte_injection"),\n        ("javascript:alert('xss')", "javascript_protocol"),\n        ("data:text/html,<script>alert(1)</script>", "data_uri_html"),\n        ("vbscript:msgbox('xss')", "vbscript_protocol"),\n        ("<img onload='alert(1)'>", "event_handler_injection"),\n        ("eval('malicious code')", "eval_injection"),\n        ("document.cookie = 'stolen'", "cookie_access"),\n        ("window.location = 'evil.com'", "location_manipulation"),\n    ]\n\n    for test_input, expected_pattern in test_cases:\n        result = await validator.validate_input(test_input, context)\n        fallback_violations = [\n            v\n            for v in result.violations\n            if v.violation_type.startswith("fallback_")\n            and expected_pattern in v.violation_type\n        ]\n        test.assert_test(\n            len(fallback_violations) > 0,\n            f"Fallback pattern detection: {expected_pattern}",\n        )\n        if fallback_violations:\n            test.assert_test(\n                fallback_violations[0].confidence_score > 0.0,\n                f"Confidence score for {expected_pattern}",\n            )\n            test.assert_test(\n                fallback_violations[0].detected_pattern is not None,\n                f"Detected pattern for {expected_pattern}",\n            )\n\n    # Test 7: Comprehensive Sanitization Testing\n    print("\n🧪 Testing comprehensive XSS vector sanitization...")\n\n    # Test 7a: Script tag sanitization\n    script_input = "<script>alert('xss')</script>Hello World"\n    script_sanitized = await validator.sanitize_input(script_input)\n    test.assert_test(\n        "<script>" not in script_sanitized.lower(),\n        "Script tags removed during sanitization",\n    )\n    test.assert_test(\n        "alert('xss')" not in script_sanitized, "Script content completely removed"\n    )\n    test.assert_test(\n        "Hello" in script_sanitized, "Safe content preserved during sanitization"\n    )\n\n    # Test 7b: Event handler sanitization\n    event_handlers = [\n        "<img onload='alert(1)' src='x'>",\n        "<div onclick='malicious()'>Click me</div>",\n        "<body onmouseover='steal_data()'>",\n        "<input onfocus='evil_function()' type='text'>",\n        "<a href='#' onmouseover='attack()'>Link</a>",\n    ]\n\n    for event_input in event_handlers:\n        event_sanitized = await validator.sanitize_input(event_input)\n        # Check that dangerous event handlers are removed or neutralized\n        dangerous_events = ["onload", "onclick", "onmouseover", "onfocus"]\n        for event in dangerous_events:\n            if event in event_input.lower():\n                test.assert_test(\n                    event not in event_sanitized.lower(),\n                    f"Event handler {event} completely removed",\n                )\n\n    # Test 7c: JavaScript protocol sanitization\n    js_protocols = [\n        "<a href='javascript:alert(1)'>Click</a>",\n        "<iframe src='javascript:void(0)'></iframe>",\n        "<img src='javascript:alert(\"xss\")'>",\n        "<form action='javascript:malicious()'>",\n        "<area href='javascript:steal_cookies()'>",\n    ]\n\n    for js_input in js_protocols:\n        js_sanitized = await validator.sanitize_input(js_input)\n\n        # Use strict validation for JavaScript protocols (focus on executable patterns)\n        dangerous_js_patterns = ["javascript:"]\n        assert_dangerous_patterns_removed(\n            js_input,\n            js_sanitized,\n            dangerous_js_patterns,\n            "JavaScript protocol sanitization",\n        )\n\n    # Test 7d: Iframe sanitization\n    iframe_inputs = [\n        "<iframe src='http://evil.com'></iframe>",\n        "<iframe src='data:text/html,<script>alert(1)</script>'></iframe>",\n        "<iframe srcdoc='<script>malicious()</script>'></iframe>",\n        "<iframe onload='attack()' src='about:blank'></iframe>",\n    ]\n\n    for iframe_input in iframe_inputs:\n        iframe_sanitized = await validator.sanitize_input(iframe_input)\n        # Use strict validation for iframe-related dangerous patterns (focus on executable)\n        dangerous_iframe_patterns = [\n            "<script",\n            "javascript:",\n            "data:text/html",\n            "onload=",\n            "onerror=",\n            "onclick=",\n        ]\n        assert_dangerous_patterns_removed(\n            iframe_input,\n            iframe_sanitized,\n            dangerous_iframe_patterns,\n            "Iframe sanitization",\n        )\n\n        # Additional check for iframe structure if it contains dangerous attributes\n        if "src=" in iframe_input.lower() or "srcdoc=" in iframe_input.lower():\n            iframe_lower = iframe_sanitized.lower()\n            # If iframe tag remains, ensure dangerous attributes are removed\n            if "<iframe" in iframe_lower:\n                dangerous_iframe_attrs = ["src=", "srcdoc=", "onload="]\n                for attr in dangerous_iframe_attrs:\n                    if attr in iframe_input.lower():\n                        test.assert_test(\n                            attr not in iframe_lower,\n                            f"Iframe dangerous attribute '{attr}' removed",\n                        )\n\n    # Test 7e: Additional XSS vectors\n    additional_vectors = [\n        "<object data='javascript:alert(1)'></object>",\n        "<embed src='javascript:malicious()'></embed>",\n        "<meta http-equiv='refresh' content='0;url=javascript:alert(1)'>",\n        "<link rel='stylesheet' href='javascript:alert(1)'>",\n        "<style>@import 'javascript:alert(1)';</style>",\n        "<svg onload='alert(1)'><circle r='10'/></svg>",\n        "<math><mi xlink:href='javascript:alert(1)'>click</mi></math>",\n    ]\n\n    for vector_input in additional_vectors:\n        vector_sanitized = await validator.sanitize_input(vector_input)\n\n        # Use strict validation for executable patterns in additional vectors\n        dangerous_vector_patterns = ["javascript:"]\n        assert_dangerous_patterns_removed(\n            vector_input,\n            vector_sanitized,\n            dangerous_vector_patterns,\n            f"Additional XSS vector ({vector_input[:30]}...)",\n        )\n\n        # Additional strict check for dangerous elements\n        dangerous_elements = ["object", "embed", "meta", "link", "style", "svg", "math"]\n        input_lower = vector_input.lower()\n        sanitized_lower = vector_sanitized.lower()\n\n        for elem in dangerous_elements:\n            elem_tag = f"<{elem}"\n            if elem_tag in input_lower:\n                test.assert_test(\n                    elem_tag not in sanitized_lower,\n                    f"Dangerous element '<{elem}>' completely removed",\n                )\n\n    # Test 7f: Mixed content sanitization\n    mixed_content = """\n    <div>Safe content</div>\n    <script>alert('xss1')</script>\n    <p onclick='malicious()'>Click me</p>\n    <a href='javascript:alert(2)'>Link</a>\n    <iframe src='http://evil.com'></iframe>\n    <span>More safe content</span>\n    """\n\n    mixed_sanitized = await validator.sanitize_input(mixed_content)\n\n    # Verify safe content is preserved\n    test.assert_test(\n        "Safe content" in mixed_sanitized, "Safe content preserved in mixed input"\n    )\n    test.assert_test(\n        "More safe content" in mixed_sanitized, "Multiple safe elements preserved"\n    )\n\n    # Use strict validation for executable patterns in mixed content\n    dangerous_mixed_patterns = ["<script", "onclick=", "javascript:"]\n    assert_dangerous_patterns_removed(\n        mixed_content,\n        mixed_sanitized,\n        dangerous_mixed_patterns,\n        "Mixed content sanitization",\n    )\n\n    print("✅ Comprehensive sanitization testing completed")\n\n    # Test 8: Query parameter validation\n    valid_params = {"metadata": {"key1": "value1", "key2": "value2"}}\n    result = await validator.validate_query_parameters(valid_params)\n    test.assert_test(result.is_valid, "Valid query parameters pass validation")\n    test.assert_test(\n        len(result.violations) == 0, "Valid query parameters have no violations"\n    )\n\n    invalid_metadata = {\n        "metadata": {f"key{i}": f"value{i}" for i in range(60)}\n    }  # Exceeds 50\n    result = await validator.validate_query_parameters(invalid_metadata)\n    test.assert_test(not result.is_valid, "Invalid query parameters fail validation")\n    metadata_violations = [\n        v for v in result.violations if v.violation_type == "metadata_fields_exceeded"\n    ]\n    test.assert_test(\n        len(metadata_violations) == 1, "Metadata field limit violation detected"\n    )\n\n    # Test 9: Confidence score aggregation\n    test.assert_test(\n        validator._calculate_overall_confidence([]) == 1.0,\n        "Empty confidence scores return 1.0",\n    )\n    test.assert_test(\n        validator._calculate_overall_confidence([0.8]) == 0.8,\n        "Single confidence score preserved",\n    )\n\n    scores = [0.9, 0.8, 0.7]\n    result = validator._calculate_overall_confidence(scores)\n    test.assert_test(\n        0.8 <= result <= 0.85, "Multiple confidence scores aggregated correctly"\n    )\n\n    # Test 10: Recommended action determination\n    test.assert_test(\n        validator._determine_recommended_action([]) == SecurityAction.ALLOW,\n        "No violations allow action",\n    )\n\n    critical_violation = SecurityViolation(\n        violation_type="test",\n        severity=SecuritySeverity.CRITICAL,\n        description="Critical test violation",\n        confidence_score=0.9,\n    )\n    test.assert_test(\n        validator._determine_recommended_action([critical_violation])\n        == SecurityAction.BLOCK,\n        "Critical violations block action",\n    )\n\n    high_conf_error = SecurityViolation(\n        violation_type="test",\n        severity=SecuritySeverity.ERROR,\n        description="High confidence error",\n        confidence_score=0.95,\n    )\n    test.assert_test(\n        validator._determine_recommended_action([high_conf_error])\n        == SecurityAction.BLOCK,\n        "High confidence errors block action",\n    )\n\n    xss_violation = SecurityViolation(\n        violation_type="xss_injection",\n        severity=SecuritySeverity.WARNING,\n        description="XSS test violation",\n        confidence_score=0.8,\n    )\n    test.assert_test(\n        validator._determine_recommended_action([xss_violation])\n        == SecurityAction.SANITIZE,\n        "XSS violations sanitize action",\n    )\n\n    # Test 11: Library health check\n    health_status = await validator.check_library_health()\n    test.assert_test(\n        isinstance(health_status, LibraryHealthStatus),\n        "Library health check returns correct type",\n    )\n    test.assert_test(\n        health_status.last_health_check is not None, "Health check timestamp recorded"\n    )\n    test.assert_test(\n        isinstance(health_status.health_check_errors, list),\n        "Health check errors is list",\n    )\n\n    status = validator.get_library_status()\n    test.assert_test(isinstance(status, dict), "Library status returns dictionary")\n    test.assert_test("libinjection" in status, "Library status includes libinjection")\n    test.assert_test("bleach" in status, "Library status includes bleach")\n    test.assert_test("markupsafe" in status, "Library status includes markupsafe")\n    test.assert_test(\n        "fallback_patterns_count" in status, "Library status includes pattern count"\n    )\n    test.assert_test(\n        status["fallback_patterns_count"] == len(SecurityValidator.HIGH_RISK_PATTERNS),\n        "Pattern count matches HIGH_RISK_PATTERNS",\n    )\n\n    # Test 12: Processing time measurement\n    test_input = "test input"\n    result = await validator.validate_input(test_input, context)\n    test.assert_test(result.processing_time_ms >= 0, "Processing time is non-negative")\n    test.assert_test(\n        isinstance(result.processing_time_ms, float), "Processing time is float"\n    )\n\n    # Test 13: Comprehensive integration scenario\n    malicious_input = """\n    <script>alert('xss')</script>\n    '; DROP TABLE users; --\n    javascript:alert('more xss')\n    {{user.password}}\n    eval('malicious code')\n    """\n\n    context = {"source_ip": "192.168.1.100", "user_agent": "Mozilla/5.0 (Test Browser)"}\n    result = await validator.validate_input(malicious_input, context)\n\n    test.assert_test(not result.is_valid, "Malicious input fails validation")\n    test.assert_test(len(result.violations) > 0, "Malicious input has violations")\n    test.assert_test(\n        result.recommended_action in [SecurityAction.BLOCK, SecurityAction.SANITIZE],\n        "Malicious input has appropriate action",\n    )\n    test.assert_test(\n        result.confidence_score > 0.0, "Malicious input has confidence score"\n    )\n\n    fallback_violations = [\n        v for v in result.violations if v.violation_type.startswith("fallback_")\n    ]\n    test.assert_test(\n        len(fallback_violations) > 0, "Malicious input detected by fallback patterns"\n    )\n\n    sanitized = await validator.sanitize_input(malicious_input)\n    test.assert_test(\n        len(sanitized) < len(malicious_input), "Sanitization reduces input size"\n    )\n    test.assert_test("<script>" not in sanitized, "Script tags removed by sanitization")\n\n    # Summary\n    test.print_summary("SecurityValidator Unit Tests")\n    return test.all_passed()\n\n\nif __name__ == "__main__":\n    success = asyncio.run(run_tests())\n    sys.exit(0 if success else 1)\n